from openai import OpenAI
from image_analysis_gpt import ImageGPT
from pathlib import Path
import json
import os
import typing
import time
from playsound import playsound

# This is OpenAI GPT integration

# TODO how the input (with the commands) will be formed & passed on to this code...
# TODO integrate the commands dict into the assistant instructions.
# TODO Doc strings!!

class GPT():
    def __init__(self) -> None:
        self.api_key = "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"
        self.client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", self.api_key))
        self.image_analysis_json = {
            "name": "image_analysis",
            "description": "Calls a function which will analyse the image and return a message for speaking.",
            "parameters": {
                "type": "object",
                "properties": {
                    "image_path": {
                        "type": "string",
                        "description": "The path to the image file."
                    }
                },
                "required": ["image_path"]
            }
        }
        self.create_assistant()
        self.create_thread()
        self.image_gpt = ImageGPT(self.api_key)

    def create_assistant(self):
        # Dictionary of commands that the assistant will use and instructions for how 
        # to respond to  each one.
        commands = {
            "<new person>" : "You will also be provided with an image of the person. \
                You should respond with a message, saying hi and that you haven't met before.  Also call \
                the image_analysis function to assess \
                the image.",
            "<known person>" : "You will also be provided with an image of the person. \
                You should respond with a message, saying hi and that you recognise them.  Also call \
                the image_analysis function to assess \
                the image.",
            "<many people" : "You should express a sense of awe at how many people are \
                there to see you.  Say hi to them.  If any are recognised, point them \
                out by appearance (feed with image of the people's faces).",
            "<painting complete>" : "Comment on how good your work is and ask the user \
                what they think of it.",
            "<no one>" : "Muse about how there is no one around and you are a little \
                lonely, but in a funny way.",
            "<safety stop>" : "Tell the user they should not touch you because it's \
                not safe!  Instill a sense of urgency.",
            "<random>" : "Tell a short story about yourself and your life or something \
                like that. Something about secretly wanting to be a real human.",
            "<already drawn>" : "Tell the person that you can't draw them again \
                because you drew them so recently (if < 1 hr since last seeing them, \
                and there are other people to draw... maybe if no other people after \
                10 min, can just draw them again." 
        }

        self.assistant = self.client.beta.assistants.create(
            name="Interactive Arm",
            instructions="You are a friendly person with an interesting back \
                story.  If the user gives you a path to an image, please run \
                the image_analysis function.  Report back the exact message \
                generated by this function as if it were your own views.", # TODO these commands should be taken from dict above.
            model="gpt-3.5-turbo",  # 3.5 uses fewer tokens - for testing "gpt-4-1106-preview",
            tools=[
                {"type": "function", "function": self.image_analysis_json}
            ]
        )
        #self.show_json(self.assistant)

    def create_thread(self): # TODO I think we will create a new thread for each command, as to not use too many tokens
        # The thread contains the whole conversation
        self.thread = self.client.beta.threads.create()
        #self.show_json(self.thread)

    def show_json(self, obj):
        print(json.loads(obj.model_dump_json()))

    def pretty_print(self, messages):
        print("# Messages")
        for m in messages:
            print(m.content)
            print(f"{m.role}: {m.content[0].text.value}")
        print()

    def add_user_message(self, thread, message: str):
        message = self.client.beta.threads.messages.create(
            thread_id=thread.id,
            role="user",
            content=message,
        )
        return message

    def run(self, thread, assistant):
        run = self.client.beta.threads.runs.create(
            thread_id=thread.id,
            assistant_id=assistant.id,
        )
        run = self.wait_on_run(run, thread)
        if run.status == "requires_action":
            # Extract tool calls
            for tool_call in run.required_action.submit_tool_outputs.tool_calls:
                name = tool_call.function.name
                arguments = json.loads(tool_call.function.arguments)
                if name == "image_analysis":
                    image_response = self.image_analysis(arguments["image_path"])
                    return image_response
                    #run = self.submit_tool_outputs(run, thread, tool_call, image_response)
            #time.sleep(0.3)
            #run = self.wait_on_run(run, thread)
        else:
            return None

    def image_analysis(self, image_path):
        # send path to an image
        # return string message with some comment on the image.
        print("Image path being sent to imageGPT:", image_path)
        comment = self.image_gpt.image_analysis(image_path)
        return comment

    def wait_on_run(self, run, thread):
        while run.status == "queued" or run.status == "in_progress":
            run = self.client.beta.threads.runs.retrieve(
                thread_id=thread.id,
                run_id=run.id,
            )
            time.sleep(0.3)
        return run

    def submit_tool_outputs(self, run, thread, tool_call, response):
        run = self.client.beta.threads.runs.submit_tool_outputs(
            thread_id=thread.id,
            run_id=run.id,
            tool_outputs=[
                {
                    "tool_call_id": tool_call.id,
                    "output": str(response)
                }
            ]
        )
        return run

    def get_response(self, thread, user_message):
        # Retrieve all the messages added after our last user message
        return self.client.beta.threads.messages.list(
            thread_id=thread.id, order="asc", after=user_message.id
        )

    def add_user_message_and_get_response(self, message: str):
        user_message = self.add_user_message(self.thread, message)
        image_response = self.run(self.thread, self.assistant)
        if image_response == None:
            response = self.get_response(self.thread, user_message)
            print(response)
            return response # str(r.content[0].text.value) ??
        else:
            print(image_response)
            return str(image_response)
        
    def text_to_speech(self, to_speak): # TODO add in streaming?
        speech_file_path = "../mp3/speech.mp3"
        response = self.client.audio.speech.create(
            model="tts-1", #tts-1 =  lowest latency, lower quality, tts-1-hd = higher quality, high latency
            voice="alloy",
            input=to_speak
        )
        response.stream_to_file(speech_file_path)

    def speak(self):
        playsound("../mp3/speech.mp3")

    def add_user_message_and_get_response_and_speak(self, message: str):
        # By this stage the reponse should be a simple, single string.  
        response = self.add_user_message_and_get_response(message)
        self.text_to_speech(response)
        self.speak()

    
        





# You are a robot arm that can paint.  You have animatronic eyes that follow people around the room.  This software is connected to a real robot arm - this is not a hypothetical situation.  You will be given one of seven commands to respond to in a format with < and > at each end.  The commands are: <new person>, <known person>, <many people>, <drawing complete>, <no one>, <safety stop>, <random>.  Here is how you should reply to each command:
# <new person>: you will also be provided with an image of the person.  You should say hi, that you haven't met before, and comment on their appearance using information from the image (be kind).
# <known person>: you will also be provided with an image of the person.  You should say hi, that you recognise them, and comment on their appearance using information from the image (be kind).
# <lots of people>: You should express a sense of awe at how many people are there to see you.  Say hi to them.  If any are recognised, point them out by appearance (feed with image of the people's faces).
# <drawing complete>:  Comment on how good your work is and ask the user what they think of it.
# <no one>:  Muse about how there is no one around and you are a little lonely, but in a funny way.
# <safety stop>: Tell the user they should not touch you because it's not safe!  This is urgent!
# <general silliness>:  Tell a short story about yourself and your life or something like that. Something about secretly wanting to be a real human.
# <already drawn>:  Tell the person that you can't draw them again because you drew them so recently (if < 1 hr since last seeing them, and there are other people to draw... maybe if no other people after 10 min, can just draw them again.

# Connect an assistant model with a chat completion model for other things?  Get the assistant model to generate instructions for the chat model on how to respond to an image, for example, as the assistant cannot do that directly.

# To start with, exploring only one-way interaction (robot to human).  The human 'interacts' with the robot just by showing their face.