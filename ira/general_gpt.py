from ira.image_analysis_gpt import ImageGPT
from openai import OpenAI
import json
import os
import time
import re
from playsound import playsound

# This is OpenAI GPT integration

# TODO how the input (with the commands) will be formed & passed on to this code...
# TODO integrate the commands dict into the assistant instructions.
# TODO Doc strings!!

class GPT():
    def __init__(self) -> None:
        self.api_key = "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"
        self.client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY", self.api_key))
        self.image_analysis_json = {
            "name": "image_analysis",
            "description": "Calls a function which will analyse the image and return a message for speaking.",
            "parameters": {
                "type": "object",
                "properties": {
                    "image_path": {
                        "type": "string",
                        "description": "The path to the image file."
                    }
                },
                "required": ["image_path"]
            }
        }
        self.create_assistant()
        self.create_thread()
        self.image_gpt = ImageGPT(self.api_key)

    def create_assistant(self):
        # Dictionary of commands that the assistant will use and instructions for how 
        # to respond to  each one.
        commands = {
            "<found_noone>" : "There is no one around me to paint and I am a little lonely.",
            "<say_painted_recently>" : "The person in front of me was painted too recently for \
                me to paint them again just yet.",
            "<too_far>" : "All the people I can see are too far away for me to be able to paint them. \
                They need to come closer if they want their beautiful faces painted.",
            "<interaction_unknown>" : "I am provided with an image of a person. \
                I have not met them before and I say hello.  Also call \
                the image_analysis function to assess the image.",
            "<interaction_known>" : "I am provided with an image of a person. \
                I have met them before and hence I recognise them.  Also call \
                the image_analysis function to assess the image.",
            "<interaction_known_recent>" : "I am provided with an image of a person. \
                I have met them before and hence I recognise them.  Also they \
                disappeared before I could paint them last time.  Also call \
                the image_analysis function to assess the image.",
            "<disappeared>" : "The person I was just talking to has disappeared. \
                I wonder where they could have gone to.",
            "<interaction_returned>" : "A person who disappeared before has now \
                returned.  I have joy at seeing them again and getting to continue \
                the interaction.",
            "<gone>" : "The person I was just talking to has gone for good. \
                I gracefully accept the rejection and prepare to move on to another \
                subject.",
            "<painting>" : "I am now painting you.  Comment on my love of the \
                process of painting, and that I cannot wait for them to see the final product.",
            "<continue_painting>" : "I am still painting.  Comment on how it is going.",    
            "<completed>" : "Comment on how good my work is and ask the user \
                what they think of it."
        }

        self.assistant = self.client.beta.assistants.create(
            name="Interactive Arm",
            instructions=f"""You are a friendly painting robot. \
                You will be given a command in the format <abc>; the \
                command is enclosed within the '<' and '>'.  The commands let you know \
                what is going on with people around you: whether there are people around you or not, \
                whether you painted someone recently, if they disappeared, etc.
                The commands are outlined this dictionary: {commands} \
                Do NOT respond with the exact text from the dictionary - the \
                dictionary values simply give a framework; repsond with a variation. \
                What you say should be different every time.
                Remember you are the one doing the painting, so use first person "I" for the painter \
                and second person "YOU" for the subject being painted; e.g. "Now I will paint YOU."
                If the user gives you a path to an image, please run \
                the image_analysis function.  Report back the exact message \
                generated by this function as if it were your own views.""",
            model="gpt-3.5-turbo",  # 3.5 uses fewer tokens - for testing "gpt-4-1106-preview",
            tools=[
                {"type": "function", "function": self.image_analysis_json}
            ]
        )
        self.show_json(self.assistant)

    def create_thread(self): # TODO I think we will create a new thread for each command, as to not use too many tokens
        # The thread contains the whole conversation
        self.thread = self.client.beta.threads.create()
        self.show_json(self.thread)

    def show_json(self, obj):
        print(json.loads(obj.model_dump_json()))

    def pretty_print(self, messages):
        print("# Messages")
        for m in messages:
            print(m.content)
            print(f"{m.role}: {m.content[0].text.value}")
        print()

    def add_user_message(self, thread, message: str):
        message = self.client.beta.threads.messages.create(
            thread_id=thread.id,
            role="user",
            content=message,
        )
        return message

    def run(self, thread, assistant):
        run = self.client.beta.threads.runs.create(
            thread_id=thread.id,
            assistant_id=assistant.id,
        )
        run = self.wait_on_run(run, thread)
        if run.status == "requires_action":
            # Extract tool calls
            for tool_call in run.required_action.submit_tool_outputs.tool_calls:
                name = tool_call.function.name
                arguments = json.loads(tool_call.function.arguments)
                if name == "image_analysis":
                    image_response = self.image_analysis(arguments["image_path"])
                    if run.status != "completed":
                        # If the run did not complete naturally, force it to end
                        self.client.beta.threads.runs.cancel(run_id=run.id, thread_id=thread.id)
                    return image_response
                    #run = self.submit_tool_outputs(run, thread, tool_call, image_response)
            #time.sleep(0.3)
            #run = self.wait_on_run(run, thread)
        else:
            return None

    def image_analysis(self, image_path):
        # send path to an image
        # return string message with some comment on the image.
        print("Image path being sent to imageGPT:", image_path)
        comment = self.image_gpt.image_analysis(image_path)
        return comment

    def wait_on_run(self, run, thread):
        print(f"RUN STATUS: {run.status}")
        while run.status == "queued" or run.status == "in_progress":
            run = self.client.beta.threads.runs.retrieve(
                thread_id=thread.id,
                run_id=run.id,
            )
            time.sleep(0.3)
        print(f"RUN STATUS: {run.status}")
        return run

    def submit_tool_outputs(self, run, thread, tool_call, response):
        run = self.client.beta.threads.runs.submit_tool_outputs(
            thread_id=thread.id,
            run_id=run.id,
            tool_outputs=[
                {
                    "tool_call_id": tool_call.id,
                    "output": str(response)
                }
            ]
        )
        return run

    def get_response(self, thread, user_message):
        # Retrieve all the messages added after our last user message
        return self.client.beta.threads.messages.list(
            thread_id=thread.id, order="asc", after=user_message.id
        )

    def add_user_message_and_get_response(self, message: str):
        user_message = self.add_user_message(self.thread, message)
        print(f"USER MESSAGE: {user_message}")
        image_response = self.run(self.thread, self.assistant)
        if image_response == None:
            print("HERE")
            response = self.get_response(self.thread, user_message)
            print(f"RESPONSE1: {response}")
            # Convert response to a string
            response_str = str(response)
            # Define a regular expression pattern to find the value
            pattern = r'value=(?:"([^"]*)"|\'([^\']*)\')'
            # Search for the pattern in the response string
            matches = re.findall(pattern, response_str)
            # Extract the values from the matches
            extracted_values = [match[0] if match[0] else match[1] for match in matches]
            # If you want to get the first match only
            # If you want to get the first match only
            if extracted_values:
                first_value = extracted_values[0]
                print("First Extracted Value:", first_value)
            else:
                print("No value found in the response.")
            print(f"RESPONSE2: {first_value}")
            return first_value
        else:
            print(image_response) #TODO this needs more parsing?
            return str(image_response)
        
    def text_to_speech(self, to_speak): # TODO add in streaming?
        speech_file_path = "/home/emma/ira_ws/src/ira/ira/mp3/speech.mp3" #TODO get from constants.py file
        with self.client.audio.speech.with_streaming_response.create(
            model="tts-1",
            voice="alloy",
            input=to_speak,
        ) as response:
            response.stream_to_file(speech_file_path)

    def speak(self):
        playsound("/home/emma/ira_ws/src/ira/ira/mp3/speech.mp3")

    def add_user_message_and_get_response_and_speak(self, message: str):
        # By this stage the reponse should be a simple, single string.  
        response = self.add_user_message_and_get_response(message)
        print(f"RESPONSE: {response}")
        self.text_to_speech(response)
        self.speak()
        return str(response)

    
        





# You are a robot arm that can paint.  You have animatronic eyes that follow people around the room.  This software is connected to a real robot arm - this is not a hypothetical situation.  You will be given one of seven commands to respond to in a format with < and > at each end.  The commands are: <new person>, <known person>, <many people>, <drawing complete>, <no one>, <safety stop>, <random>.  Here is how you should reply to each command:
# <new person>: you will also be provided with an image of the person.  You should say hi, that you haven't met before, and comment on their appearance using information from the image (be kind).
# <known person>: you will also be provided with an image of the person.  You should say hi, that you recognise them, and comment on their appearance using information from the image (be kind).
# <lots of people>: You should express a sense of awe at how many people are there to see you.  Say hi to them.  If any are recognised, point them out by appearance (feed with image of the people's faces).
# <drawing complete>:  Comment on how good your work is and ask the user what they think of it.
# <no one>:  Muse about how there is no one around and you are a little lonely, but in a funny way.
# <safety stop>: Tell the user they should not touch you because it's not safe!  This is urgent!
# <general silliness>:  Tell a short story about yourself and your life or something like that. Something about secretly wanting to be a real human.
# <already drawn>:  Tell the person that you can't draw them again because you drew them so recently (if < 1 hr since last seeing them, and there are other people to draw... maybe if no other people after 10 min, can just draw them again.

# Connect an assistant model with a chat completion model for other things?  Get the assistant model to generate instructions for the chat model on how to respond to an image, for example, as the assistant cannot do that directly.

# To start with, exploring only one-way interaction (robot to human).  The human 'interacts' with the robot just by showing their face.